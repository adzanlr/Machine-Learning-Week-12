{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "t3_w12_modern_cnn_batchnorm.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNYNR3l1dMjwyYH8JR4DbXw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adzanlr/Machine-Learning-Week-12/blob/main/t3_w12_modern_cnn_batchnorm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.5. Batch Normalization\n",
        "\n",
        "Source : https://d2l.ai/chapter_convolutional-modern/batch-norm.html\n",
        "\n",
        "7.5.1. Training Deep Networks\n",
        "\n",
        "the math\n",
        "\n",
        "7.5.2. Batch Normalization Layers\n",
        "\n",
        "Batch normalization implementations for fully-connected layers and convolutional layers are slightly different. We discuss both cases below.\n",
        "\n",
        "Recall that one key differences between batch normalization and other layers is that because batch normalization operates on a full minibatch at a time, we cannot just ignore the batch dimension as we did before when introducing other layers.\n",
        "\n",
        "7.5.2.1. Fully-Connected Layers\n",
        "\n",
        "When applying batch normalization to fully-connected layers, the original paper inserts batch normalization after the affine transformation and before the nonlinear activation function (later applications may insert batch normalization right after activation functions) [Ioffe & Szegedy, 2015]. Denoting the input to the fully-connected layer by x , the affine transformation by Wx+b (with the weight parameter W and the bias parameter b ), and the activation function by ϕ , we can express the computation of a batch-normalization-enabled, fully-connected layer output h as follows:\n",
        "\n",
        "(7.5.3) h=ϕ(BN(Wx+b)).\n",
        "\n",
        "Recall that mean and variance are computed on the same minibatch on which the transformation is applied.\n",
        "\n",
        "7.5.2.2. Convolutional Layers\n",
        "\n",
        "Similarly, with convolutional layers, we can apply batch normalization after the convolution and before the nonlinear activation function.\n",
        "\n",
        "When the convolution has multiple output channels, we need to carry out batch normalization for each of the outputs of these channels, and each channel has its own scale and shift parameters, both of which are scalars.\n",
        "\n",
        "Assume that our minibatches contain m examples and that for each channel, the output of the convolution has height p and width q . For convolutional layers, we carry out each batch normalization over the m⋅p⋅q elements per output channel simultaneously.\n",
        "\n",
        "Thus, we collect the values over all spatial locations when computing the mean and variance and consequently apply the same mean and variance within a given channel to normalize the value at each spatial location.\n",
        "\n",
        "7.5.2.3. Batch Normalization During Prediction\n",
        "\n",
        "As we mentioned earlier, batch normalization typically behaves differently in training mode and prediction (test) mode. First, the noise in the sample mean and the sample variance arising from estimating each on minibatches are no longer desirable once we have trained the model. Second, we might not have the luxury of computing per-batch normalization statistics. For example, we might need to apply our model to make one prediction at a time.\n",
        "\n",
        "Typically, after training, we use the entire dataset to compute stable estimates of the variable statistics and then fix them at prediction time. Consequently, batch normalization behaves differently during training and at test time. Recall that dropout also exhibits this characteristic.\n",
        "\n",
        "7.5.3. Implementation from Scratch\n",
        "\n",
        "Below, we implement a batch normalization layer with tensors from scratch.\n",
        "\n",
        "We can now create a proper BatchNorm layer. Our layer will maintain proper parameters for scale gamma and shift beta, both of which will be updated in the course of training. Additionally, our layer will maintain moving averages of the means and variances for subsequent use during model prediction."
      ],
      "metadata": {
        "id": "55V7s-2YCmT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l==0.17.0"
      ],
      "metadata": {
        "id": "Itf6tPVLCn1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting d2l==0.17.0\n",
        "  Downloading d2l-0.17.0-py3-none-any.whl (83 kB)\n",
        "     |████████████████████████████████| 83 kB 1.1 MB/s \n",
        "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from d2l==0.17.0) (3.2.2)\n",
        "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from d2l==0.17.0) (1.1.5)\n",
        "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from d2l==0.17.0) (2.23.0)\n",
        "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from d2l==0.17.0) (1.19.5)\n",
        "Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from d2l==0.17.0) (1.0.0)\n",
        "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.17.0) (5.6.1)\n",
        "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.17.0) (7.6.5)\n",
        "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.17.0) (5.3.1)\n",
        "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.17.0) (5.2.2)\n",
        "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.17.0) (5.2.0)\n",
        "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.17.0) (4.10.1)\n",
        "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==0.17.0) (5.1.1)\n",
        "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==0.17.0) (5.1.1)\n",
        "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==0.17.0) (5.3.5)\n",
        "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==0.17.0) (5.5.0)\n",
        "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.17.0) (57.4.0)\n",
        "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.17.0) (0.7.5)\n",
        "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.17.0) (4.4.2)\n",
        "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.17.0) (4.8.0)\n",
        "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.17.0) (0.8.1)\n",
        "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.17.0) (1.0.18)\n",
        "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.17.0) (2.6.1)\n",
        "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->d2l==0.17.0) (1.15.0)\n",
        "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->d2l==0.17.0) (0.2.5)\n",
        "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==0.17.0) (5.1.3)\n",
        "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==0.17.0) (1.0.2)\n",
        "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==0.17.0) (3.5.2)\n",
        "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==0.17.0) (0.2.0)\n",
        "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.17.0) (4.9.1)\n",
        "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.17.0) (4.3.3)\n",
        "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.17.0) (5.4.0)\n",
        "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.17.0) (21.4.0)\n",
        "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.17.0) (0.18.0)\n",
        "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.17.0) (4.10.0)\n",
        "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.17.0) (3.10.0.2)\n",
        "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.17.0) (3.7.0)\n",
        "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==0.17.0) (0.12.1)\n",
        "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==0.17.0) (1.8.0)\n",
        "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==0.17.0) (2.11.3)\n",
        "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->d2l==0.17.0) (2.8.2)\n",
        "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->d2l==0.17.0) (22.3.0)\n",
        "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==0.17.0) (0.7.0)\n",
        "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->d2l==0.17.0) (2.0.1)\n",
        "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==0.17.0) (0.11.0)\n",
        "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==0.17.0) (3.0.6)\n",
        "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==0.17.0) (1.3.2)\n",
        "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.17.0) (0.8.4)\n",
        "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.17.0) (1.5.0)\n",
        "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.17.0) (0.5.0)\n",
        "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.17.0) (0.3)\n",
        "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.17.0) (0.7.1)\n",
        "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.17.0) (4.1.0)\n",
        "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l==0.17.0) (21.3)\n",
        "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l==0.17.0) (0.5.1)\n",
        "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->d2l==0.17.0) (2018.9)\n",
        "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l==0.17.0) (2.0.0)\n",
        "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==0.17.0) (1.24.3)\n",
        "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==0.17.0) (2.10)\n",
        "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==0.17.0) (2021.10.8)\n",
        "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==0.17.0) (3.0.4)\n",
        "Installing collected packages: d2l\n",
        "Successfully installed d2l-0.17.0"
      ],
      "metadata": {
        "id": "eDK7rMfXDQmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "\n",
        "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
        "    # Use `is_grad_enabled` to determine whether the current mode is training\n",
        "    # mode or prediction mode\n",
        "    if not torch.is_grad_enabled():\n",
        "        # If it is prediction mode, directly use the mean and variance\n",
        "        # obtained by moving average\n",
        "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
        "    else:\n",
        "        assert len(X.shape) in (2, 4)\n",
        "        if len(X.shape) == 2:\n",
        "            # When using a fully-connected layer, calculate the mean and\n",
        "            # variance on the feature dimension\n",
        "            mean = X.mean(dim=0)\n",
        "            var = ((X - mean) ** 2).mean(dim=0)\n",
        "        else:\n",
        "            # When using a two-dimensional convolutional layer, calculate the\n",
        "            # mean and variance on the channel dimension (axis=1). Here we\n",
        "            # need to maintain the shape of `X`, so that the broadcasting\n",
        "            # operation can be carried out later\n",
        "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
        "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
        "        # In training mode, the current mean and variance are used for the\n",
        "        # standardization\n",
        "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
        "        # Update the mean and variance using moving average\n",
        "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
        "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
        "    Y = gamma * X_hat + beta  # Scale and shift\n",
        "    return Y, moving_mean.data, moving_var.data"
      ],
      "metadata": {
        "id": "tKRV-8flCpcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now create a proper BatchNorm layer. Our layer will maintain proper parameters for scale gamma and shift beta, both of which will be updated in the course of training. Additionally, our layer will maintain moving averages of the means and variances for subsequent use during model prediction."
      ],
      "metadata": {
        "id": "s6n1-SZ3DSzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNorm(nn.Module):\n",
        "    # `num_features`: the number of outputs for a fully-connected layer\n",
        "    # or the number of output channels for a convolutional layer. `num_dims`:\n",
        "    # 2 for a fully-connected layer and 4 for a convolutional layer\n",
        "    def __init__(self, num_features, num_dims):\n",
        "        super().__init__()\n",
        "        if num_dims == 2:\n",
        "            shape = (1, num_features)\n",
        "        else:\n",
        "            shape = (1, num_features, 1, 1)\n",
        "        # The scale parameter and the shift parameter (model parameters) are\n",
        "        # initialized to 1 and 0, respectively\n",
        "        self.gamma = nn.Parameter(torch.ones(shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(shape))\n",
        "        # The variables that are not model parameters are initialized to 0 and 1\n",
        "        self.moving_mean = torch.zeros(shape)\n",
        "        self.moving_var = torch.ones(shape)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # If `X` is not on the main memory, copy `moving_mean` and\n",
        "        # `moving_var` to the device where `X` is located\n",
        "        if self.moving_mean.device != X.device:\n",
        "            self.moving_mean = self.moving_mean.to(X.device)\n",
        "            self.moving_var = self.moving_var.to(X.device)\n",
        "        # Save the updated `moving_mean` and `moving_var`\n",
        "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
        "            X, self.gamma, self.beta, self.moving_mean,\n",
        "            self.moving_var, eps=1e-5, momentum=0.9)\n",
        "        return Y"
      ],
      "metadata": {
        "id": "H_2L4SFFCr8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.5.4. Applying Batch Normalization in LeNet\n",
        "\n",
        "To see how to apply BatchNorm in context, below we apply it to a traditional LeNet model (Section 6.6). Recall that batch normalization is applied after the convolutional layers or fully-connected layers but before the corresponding activation functions."
      ],
      "metadata": {
        "id": "Qj-fFUtWDVyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.Sequential(\n",
        "    nn.Conv2d(1, 6, kernel_size=5), \n",
        "    BatchNorm(6, num_dims=4), \n",
        "    nn.Sigmoid(),\n",
        "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "    nn.Conv2d(6, 16, kernel_size=5), \n",
        "    BatchNorm(16, num_dims=4), nn.Sigmoid(),\n",
        "    nn.AvgPool2d(kernel_size=2, stride=2), \n",
        "    nn.Flatten(),\n",
        "    nn.Linear(16*4*4, 120), \n",
        "    BatchNorm(120, num_dims=2), \n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(120, 84), \n",
        "    BatchNorm(84, num_dims=2), \n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(84, 10))"
      ],
      "metadata": {
        "id": "n5iB_lvRCtMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, we will train our network on the Fashion-MNIST dataset. This code is virtually identical to that when we first trained LeNet (Section 6.6). The main difference is the larger learning rate."
      ],
      "metadata": {
        "id": "pFZ8t3nuDaVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr, num_epochs, batch_size = 1.0, 10, 256\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
        "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
      ],
      "metadata": {
        "id": "trPH5tTWCw_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss 0.267, train acc 0.901, test acc 0.786\n",
        "21544.3 examples/sec on cuda:0\n",
        "Let us have a look at the scale parameter gamma and the shift parameter beta learned from the first batch normalization layer."
      ],
      "metadata": {
        "id": "flRHgNokDdLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net[1].gamma.reshape((-1,)), net[1].beta.reshape((-1,))"
      ],
      "metadata": {
        "id": "Kpb7ayAkCyPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 60000 entries, 0 to 59999\n",
        "Columns: 785 entries, label to pixel784\n",
        "dtypes: int64(785)\n",
        "memory usage: 359.3 MB\n",
        "None\n",
        "   label  pixel1  pixel2  pixel3  ...  pixel781  pixel782  pixel783  pixel784\n",
        "0      2       0       0       0  ...         0         0         0         0\n",
        "1      9       0       0       0  ...         0         0         0         0\n",
        "2      6       0       0       0  ...         0         0         0         0\n",
        "3      0       0       0       0  ...         0         0         0         0\n",
        "4      3       0       0       0  ...         0         0         0         0\n",
        "\n",
        "[5 rows x 785 columns]\n",
        "To build our own dataset, need to create a class that inherits from the Dataset. Besides, function get_item() & len() must be defined at least\n",
        "\n",
        "get_item() return the specified image and its label len() return the number of dataset"
      ],
      "metadata": {
        "id": "T67VuVk_C0cT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.Sequential(\n",
        "    nn.Conv2d(1, 6, kernel_size=5), \n",
        "    nn.BatchNorm2d(6), \n",
        "    nn.Sigmoid(),\n",
        "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "    nn.Conv2d(6, 16, kernel_size=5), \n",
        "    nn.BatchNorm2d(16), \n",
        "    nn.Sigmoid(),\n",
        "    nn.AvgPool2d(kernel_size=2, stride=2), \n",
        "    nn.Flatten(),\n",
        "    nn.Linear(256, 120), \n",
        "    nn.BatchNorm1d(120), \n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(120, 84), \n",
        "    nn.BatchNorm1d(84), \n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(84, 10))"
      ],
      "metadata": {
        "id": "FKejeSMiC2Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we use the same hyperparameters to train our model. Note that as usual, the high-level API variant runs much faster because its code has been compiled to C++ or CUDA while our custom implementation must be interpreted by Python."
      ],
      "metadata": {
        "id": "SJCoDYh4DqXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
      ],
      "metadata": {
        "id": "NeNiT6cxDqyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss 0.268, train acc 0.902, test acc 0.853\n",
        "33000.3 examples/sec on cuda:0\n",
        "7.5.7. Summary\n",
        "\n",
        "During model training, batch normalization continuously adjusts the intermediate output of the neural network by utilizing the mean and standard deviation of the minibatch, so that the values of the intermediate output in each layer throughout the neural network are more stable.\n",
        "\n",
        "The batch normalization methods for fully-connected layers and convolutional layers are slightly different.\n",
        "\n",
        "Like a dropout layer, batch normalization layers have different computation results in training mode and prediction mode.\n",
        "\n",
        "Batch normalization has many beneficial side effects, primarily that of regularization. On the other hand, the original motivation of reducing internal covariate shift seems not to be a valid explanation."
      ],
      "metadata": {
        "id": "MF_zzR9mDrvr"
      }
    }
  ]
}